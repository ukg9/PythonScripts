import xgboost as xgb
import pandas as pd
from sklearn.preprocessing import LabelEncoder
import numpy as np

# List of continuous and categorical variables
continuous_vars = ['Cont1', 'Cont2', 'Cont3', 'Cont4', 'Cont5', 'Cont6', 'Cont7', 'Cont8', 'Cont9', 'Cont10', 'Cont11', 'Cont12', 'Cont13', 'Cont14']
categorical_vars = ['Cat1', 'Cat2', 'Cat3', 'Cat4', 'Cat5']

# Create interaction variables for continuous variables
for i in range(len(continuous_vars)):
    for j in range(i+1, len(continuous_vars)):
        col_name = f"{continuous_vars[i]}_{continuous_vars[j]}"
        leaf[col_name] = leaf[continuous_vars[i]] * leaf[continuous_vars[j]]

# Encode categorical variables
for col in categorical_vars:
    le = LabelEncoder()
    leaf[col] = le.fit_transform(leaf[col])

# Create interaction variables for categorical variables
for i in range(len(categorical_vars)):
    for j in range(i+1, len(categorical_vars)):
        col_name = f"{categorical_vars[i]}_{categorical_vars[j]}"
        leaf[col_name] = leaf[categorical_vars[i]].astype(str) + "_" + leaf[categorical_vars[j]].astype(str)
        le = LabelEncoder()
        leaf[col_name] = le.fit_transform(leaf[col_name])

# Create interaction variables for continuous and categorical variables
for cont in continuous_vars:
    for cat in categorical_vars:
        col_name = f"{cont}_{cat}"
        leaf[col_name] = leaf[cont] * leaf[cat]

# Separate the dependent variable
y = leaf['DNBScore']
X = leaf.drop(columns=['DNBScore'])

# Convert the DataFrame to DMatrix
data_dmatrix = xgb.DMatrix(data=X, label=y)

# Train the model
params = {
    'objective': 'reg:squarederror',  # Objective for regression task
    'max_depth': 3,                   # Maximum depth of trees
    'learning_rate': 0.1,             # Learning rate
    'n_estimators': 100,              # Number of boosting rounds
}

# Train the model on the entire dataset
model = xgb.train(params, data_dmatrix, num_boost_round=100)

# Get feature importance scores based on gain
importance_scores = model.get_score(importance_type='gain')
sorted_importance = {k: v for k, v in sorted(importance_scores.items(), key=lambda item: item[1], reverse=True)}

# Print the feature importance scores
print("Feature Importance Scores (Gain):", sorted_importance)
