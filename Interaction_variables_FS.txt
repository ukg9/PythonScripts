1. Polynomial Features

from sklearn.preprocessing import PolynomialFeatures

# Example dataset
X = ...  # Your feature matrix

# Create polynomial and interaction features
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X)

# Convert to DMatrix
data_dmatrix = xgb.DMatrix(data=X_poly, label=y)

# Train XGBoost model
model = xgb.train(params, data_dmatrix, num_boost_round=100)

# Get feature importance scores
importance_scores = model.get_score(importance_type='gain')
sorted_importance = {k: v for k, v in sorted(importance_scores.items(), key=lambda item: item[1], reverse=True)}

# Print feature importance scores
print("Feature Importance Scores (Gain):", sorted_importance)


2. Automatic Feature Engineering
import featuretools as ft

# Example dataset
X = ...  # Your feature matrix
y = ...  # Your target variable

# Create an entity set and add the dataframe
es = ft.EntitySet(id='data')
es.entity_from_dataframe(entity_id='leaf', dataframe=X, index='index')

# Create new features using deep feature synthesis
features, feature_names = ft.dfs(entityset=es, target_entity='leaf', max_depth=2)

# Convert to DMatrix
data_dmatrix = xgb.DMatrix(data=features, label=y)

# Train XGBoost model
model = xgb.train(params, data_dmatrix, num_boost_round=100)

# Get feature importance scores
importance_scores = model.get_score(importance_type='gain')
sorted_importance = {k: v for k, v in sorted(importance_scores.items(), key=lambda item: item[1], reverse=True)}

# Print feature importance scores
print("Feature Importance Scores (Gain):", sorted_importance)
