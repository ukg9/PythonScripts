Steps to Identify Significant Interaction Terms
Generate Interaction Terms: Create interaction terms between all pairs of variables.
Build the Model: Fit a regression model that includes both main effects and interaction terms.
Feature Selection: Use feature selection techniques to identify significant interaction terms.


Step 1: Generate Interaction Terms
Using itertools.combinations, generate all pairs of variables and create interaction terms.

import pandas as pd
from itertools import combinations

# Sample DataFrame (replace with your actual DataFrame)
data = {
    'DNBScore': [70, 85, 78, 95, 88],
    'Cont1': [1.5, 2.3, 3.1, 4.0, 2.2],
    'Cont2': [3.2, 4.5, 1.3, 2.2, 3.8],
    'Cat1': ['A', 'B', 'A', 'B', 'A'],
    'Cat2': ['X', 'Y', 'X', 'Y', 'X']
}
df = pd.DataFrame(data)

# List of continuous and categorical variables
continuous_vars = ['Cont1', 'Cont2']
categorical_vars = ['Cat1', 'Cat2']

# One-hot encode categorical variables
df_encoded = pd.get_dummies(df, columns=categorical_vars, drop_first=True)

# Define independent variables
independent_vars = continuous_vars + list(df_encoded.columns.difference(['DNBScore']))
X = df_encoded[independent_vars]
y = df_encoded['DNBScore']



Step 2: Create Interaction Terms
Generate interaction terms for all pairs of independent variables.

def create_interaction_terms(df, independent_vars):
    interaction_terms = pd.DataFrame()
    for var1, var2 in combinations(independent_vars, 2):
        interaction_term_name = f"{var1}_{var2}"
        interaction_terms[interaction_term_name] = df[var1] * df[var2]
    return interaction_terms

# Generate interaction terms
interaction_terms = create_interaction_terms(X, independent_vars)
X_with_interactions = pd.concat([X, interaction_terms], axis=1)



Step 3: Fit the Model with Interaction Terms
Fit a regression model that includes the interaction terms.

import statsmodels.api as sm

# Add a constant to the model
X_with_interactions = sm.add_constant(X_with_interactions)

# Fit the OLS model
model = sm.OLS(y, X_with_interactions).fit()
print(model.summary())



Step 4: Feature Selection
Use feature selection techniques to identify significant interaction terms. statsmodels and scikit-learn offer various methods for feature selection.

Using statsmodels for p-value Thresholding

# Extract the summary of the model
summary = model.summary2().tables[1]

# Filter for significant interaction terms (p-value < 0.05)
significant_interactions = summary[summary['P>|t|'] < 0.05]

print("Significant Interaction Terms:")
print(significant_interactions)

***********Using scikit-learn's RFE (Recursive Feature Elimination)***************
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression

# Initialize the model
lm = LinearRegression()
selector = RFE(lm, n_features_to_select=10, step=1)  # Select the top 10 features

# Fit RFE
selector = selector.fit(X_with_interactions, y)

# Get the ranking of features
ranking = pd.Series(selector.ranking_, index=X_with_interactions.columns)
significant_features = ranking[ranking == 1]

print("Significant Features after RFE:")
print(significant_features)



***********************************************************************************************8
COMBINED WORKFLOW

import pandas as pd
from itertools import combinations
import statsmodels.api as sm
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression

# Sample DataFrame (replace with your actual DataFrame)
data = {
    'DNBScore': [70, 85, 78, 95, 88],
    'Cont1': [1.5, 2.3, 3.1, 4.0, 2.2],
    'Cont2': [3.2, 4.5, 1.3, 2.2, 3.8],
    'Cat1': ['A', 'B', 'A', 'B', 'A'],
    'Cat2': ['X', 'Y', 'X', 'Y', 'X']
}
df = pd.DataFrame(data)

# One-hot encode categorical variables
categorical_vars = ['Cat1', 'Cat2']
df_encoded = pd.get_dummies(df, columns=categorical_vars, drop_first=True)

# Define independent variables
continuous_vars = ['Cont1', 'Cont2']
independent_vars = continuous_vars + list(df_encoded.columns.difference(['DNBScore']))
X = df_encoded[independent_vars]
y = df_encoded['DNBScore']

# Generate interaction terms
def create_interaction_terms(df, independent_vars):
    interaction_terms = pd.DataFrame()
    for var1, var2 in combinations(independent_vars, 2):
        interaction_term_name = f"{var1}_{var2}"
        interaction_terms[interaction_term_name] = df[var1] * df[var2]
    return interaction_terms

interaction_terms = create_interaction_terms(X, independent_vars)
X_with_interactions = pd.concat([X, interaction_terms], axis=1)

# Add a constant to the model
X_with_interactions = sm.add_constant(X_with_interactions)

# Fit the OLS model
model = sm.OLS(y, X_with_interactions).fit()
print(model.summary())

# Feature selection using statsmodels
summary = model.summary2().tables[1]
significant_interactions = summary[summary['P>|t|'] < 0.05]
print("Significant Interaction Terms:")
print(significant_interactions)

# Feature selection using RFE
lm = LinearRegression()
selector = RFE(lm, n_features_to_select=10, step=1)
selector = selector.fit(X_with_interactions, y)
ranking = pd.Series(selector.ranking_, index=X_with_interactions.columns)
significant_features = ranking[ranking == 1]
print("Significant Features after RFE:")
print(significant_features)
