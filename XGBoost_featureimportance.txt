import pandas as pd
import xgboost as xgb
from sklearn.preprocessing import OneHotEncoder
import matplotlib.pyplot as plt

# Separate features and target
X = df.drop(columns='DNB')
y = df['DNB']

# Encode categorical variables
X = pd.get_dummies(X, drop_first=True)

# Create DMatrix for the entire dataset
dtrain = xgb.DMatrix(X, label=y)

# Define parameters for XGBoost
params = {
    "objective": "reg:squarederror",
    "max_depth": 4,
    "eta": 0.1,
    "eval_metric": "rmse"
}

# Train the model using the entire dataset
model = xgb.train(params, dtrain, num_boost_round=100)

# Get feature importance
importance = model.get_score(importance_type='weight')

# Convert feature importance into a DataFrame for better visualization
importance_df = pd.DataFrame({'Feature': list(importance.keys()), 'Importance': list(importance.values())})

# Plot feature importance
importance_df.sort_values(by='Importance', ascending=False, inplace=True)
plt.barh(importance_df['Feature'], importance_df['Importance'])
plt.xlabel('Feature Importance')
plt.ylabel('Feature')
plt.title('Feature Importance using XGBoost')
plt.show()

# Print feature importance DataFrame
print(importance_df)


************************************************
# Separate features and target
X = df.drop(columns=['Target'])
y = df['Target']

# Apply OneHotEncoding to the categorical column (Feature2)
encoder = OneHotEncoder(sparse=False)
X_encoded = pd.DataFrame(encoder.fit_transform(X[['Feature2']]), columns=encoder.get_feature_names_out())

# Merge the one-hot encoded columns back with the original dataset
X = X.drop(columns=['Feature2'])
X = pd.concat([X, X_encoded], axis=1)

# Train an XGBoost model
model = xgb.XGBClassifier()
model.fit(X, y)

# Explain the model using SHAP
explainer = shap.Explainer(model)
shap_values = explainer(X)

# Plot SHAP summary
shap.summary_plot(shap_values, X)
