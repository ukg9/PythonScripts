import pandas as pd
import xgboost as xgb
from sklearn.preprocessing import OneHotEncoder
import matplotlib.pyplot as plt

# Separate features and target
X = df.drop(columns='CScore')
y = df['CScore']

# Encode categorical variables
X = pd.get_dummies(X, drop_first=True)

# Create DMatrix for the entire dataset
dtrain = xgb.DMatrix(X, label=y)

# Define parameters for XGBoost
params = {
    "objective": "reg:squarederror",
    "max_depth": 4,
    "eta": 0.1,
    "eval_metric": "rmse"
}

# Train the model using the entire dataset
model = xgb.train(params, dtrain, num_boost_round=100)

# Get feature importance
importance = model.get_score(importance_type='weight')

# Convert feature importance into a DataFrame for better visualization
importance_df = pd.DataFrame({'Feature': list(importance.keys()), 'Importance': list(importance.values())})

# Plot feature importance
importance_df.sort_values(by='Importance', ascending=False, inplace=True)
plt.barh(importance_df['Feature'], importance_df['Importance'])
plt.xlabel('Feature Importance')
plt.ylabel('Feature')
plt.title('Feature Importance using XGBoost')
plt.show()

# Print feature importance DataFrame
print(importance_df)